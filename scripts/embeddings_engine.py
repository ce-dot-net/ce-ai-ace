#!/usr/bin/env python3
"""
Hybrid Semantic Similarity Engine

Multi-tier similarity calculation with automatic fallback:
1. Claude (via Task tool) - Best quality, domain-aware semantic analysis
2. ChromaDB MCP (local) - Fast, good quality, sentence-transformers
3. Jaccard (string matching) - Emergency fallback, always works

Based on ACE paper requirement: "semantic embeddings with 0.85 similarity threshold"
"""

import json
import subprocess
import sys
from pathlib import Path
from typing import Tuple, Optional


class SemanticSimilarityEngine:
    """
    Multi-tier semantic similarity engine with automatic fallback.
    """

    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path.cwd()
        self.chromadb_available = self._check_chromadb_available()

    def _check_chromadb_available(self) -> bool:
        """Check if ChromaDB MCP is available."""
        try:
            # Check if ChromaDB MCP is configured
            # Plugin namespaces MCPs as "chromadb-ace" to avoid conflicts
            # But users might have their own "chromadb" MCP installed
            config_path = Path.home() / '.config' / 'claude-code' / 'config.json'
            if config_path.exists():
                with open(config_path) as f:
                    config = json.load(f)
                    mcp_servers = config.get('mcpServers', {})
                    # Check for plugin's namespaced version first
                    return 'chromadb-ace' in mcp_servers or 'chromadb' in mcp_servers
        except Exception:
            pass

        return False

    def calculate_similarity(self, text1: str, text2: str) -> Tuple[float, str, str]:
        """
        Calculate semantic similarity between two texts.

        Args:
            text1: First text
            text2: Second text

        Returns:
            (similarity_score, method_used, reasoning)
            - similarity_score: 0.0 to 1.0
            - method_used: 'claude' | 'chromadb' | 'jaccard'
            - reasoning: Human-readable explanation
        """
        # Tier 1: Try Claude semantic analysis (best quality)
        try:
            score, reasoning = self._claude_similarity(text1, text2)
            return (score, 'claude', reasoning)
        except Exception as e:
            print(f"âš ï¸  Claude similarity failed: {e}", file=sys.stderr)

        # Tier 2: Try ChromaDB MCP (fast, local)
        if self.chromadb_available:
            try:
                score = self._chromadb_similarity(text1, text2)
                return (score, 'chromadb', f"ChromaDB cosine similarity: {score:.2f}")
            except Exception as e:
                print(f"âš ï¸  ChromaDB similarity failed: {e}", file=sys.stderr)

        # Tier 3: Fallback to Jaccard (always works)
        score = self._jaccard_similarity(text1, text2)
        return (score, 'jaccard', f"Jaccard string similarity (fallback): {score:.2f}")

    def _claude_similarity(self, text1: str, text2: str) -> Tuple[float, str]:
        """
        Use Claude to calculate semantic similarity.

        Returns:
            (similarity_score, reasoning)
        """
        prompt = f"""Compare these two pattern descriptions semantically:

Pattern 1: {text1}

Pattern 2: {text2}

Analyze:
1. Are they describing the same coding pattern/practice?
2. Consider semantic meaning, not just word overlap
3. Domain context (e.g., "Stripe payments" vs "payment processing")

Output JSON:
{{
  "similarity": 0.0-1.0,
  "reasoning": "Brief explanation",
  "same_pattern": true/false
}}

Examples:
- "Use TypedDict for configs" vs "TypedDict for configuration" â†’ 0.95 (same pattern)
- "Stripe in services/" vs "payments with Stripe SDK" â†’ 0.7 (related, different aspect)
- "Use async/await" vs "Use TypedDict" â†’ 0.1 (unrelated)
"""

        # Call Claude via subprocess (simulates Task tool)
        # In actual Claude Code CLI, this would use the Task tool
        result = subprocess.run(
            ['python3', '-c', f'''
import json
# Simulate Claude semantic analysis
# In production, this calls Claude via Task tool
response = {{
    "similarity": 0.5,  # Placeholder
    "reasoning": "Claude analysis unavailable (stub)",
    "same_pattern": False
}}
print(json.dumps(response))
'''],
            capture_output=True,
            text=True,
            timeout=10
        )

        if result.returncode == 0:
            response = json.loads(result.stdout)
            return (response['similarity'], response['reasoning'])

        raise Exception(f"Claude call failed: {result.stderr}")

    def _chromadb_similarity(self, text1: str, text2: str) -> float:
        """
        Use ChromaDB MCP to calculate cosine similarity.

        Returns:
            similarity_score (0.0 to 1.0)
        """
        # This would call ChromaDB MCP via Claude Code's MCP bridge
        # For now, stub implementation

        # In production:
        # 1. Encode text1 and text2 with sentence-transformers
        # 2. Calculate cosine similarity
        # 3. Return score

        raise Exception("ChromaDB MCP stub - not implemented yet")

    def _jaccard_similarity(self, text1: str, text2: str) -> float:
        """
        Fallback: Jaccard similarity (string-based).

        Returns:
            similarity_score (0.0 to 1.0)
        """
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union)


def test_similarity_engine():
    """Test the similarity engine with sample patterns."""
    engine = SemanticSimilarityEngine()

    test_pairs = [
        ("Use TypedDict for configs", "TypedDict for configuration objects"),
        ("Stripe in services/stripe.ts", "Payments with Stripe SDK"),
        ("Use async/await", "Use TypedDict for configs"),
        ("Use list comprehensions", "Python list comprehensions for iteration"),
    ]

    print("ðŸ§ª Testing Semantic Similarity Engine\n")

    for text1, text2 in test_pairs:
        score, method, reasoning = engine.calculate_similarity(text1, text2)
        print(f"Pattern 1: {text1}")
        print(f"Pattern 2: {text2}")
        print(f"Similarity: {score:.2f} (method: {method})")
        print(f"Reasoning: {reasoning}")
        print("-" * 60)


if __name__ == '__main__':
    test_similarity_engine()
